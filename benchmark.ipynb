{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping /Users/pablo-admin/anaconda3/envs/ml_env/lib/python3.11/site-packages/scipy-1.13.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: tokenizers in /Users/pablo-admin/anaconda3/envs/ml_env/lib/python3.11/site-packages (0.13.3)\n",
      "\u001b[33mWARNING: Skipping /Users/pablo-admin/anaconda3/envs/ml_env/lib/python3.11/site-packages/scipy-1.13.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'CharacterTokenizer' from 'tokenizer' (/Users/pablo-admin/anaconda3/envs/ml_env/lib/python3.11/site-packages/tokenizer/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgenomic_benchmarks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloc2seq\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m download_dataset\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgenomic_benchmarks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_check\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_downloaded\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtokenizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CharacterTokenizer\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Helper Functions\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexists\u001b[39m(val):\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'CharacterTokenizer' from 'tokenizer' (/Users/pablo-admin/anaconda3/envs/ml_env/lib/python3.11/site-packages/tokenizer/__init__.py)"
     ]
    }
   ],
   "source": [
    "# Required Imports\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from genomic_benchmarks.loc2seq import download_dataset\n",
    "from genomic_benchmarks.data_check import is_downloaded\n",
    "from tokenizer import CharacterTokenizer\n",
    "# Helper Functions\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def coin_flip():\n",
    "    return random() > 0.5\n",
    "\n",
    "string_complement_map = {'A': 'T', 'C': 'G', 'G': 'C', 'T': 'A', 'a': 't', 'c': 'g', 'g': 'c', 't': 'a'}\n",
    "def string_reverse_complement(seq):\n",
    "    rev_comp = ''\n",
    "    for base in seq[::-1]:\n",
    "        if base in string_complement_map:\n",
    "            rev_comp += string_complement_map[base]\n",
    "        else:\n",
    "            rev_comp += base\n",
    "    return rev_comp\n",
    "\n",
    "# Dataset Class\n",
    "class GenomicBenchmarkDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, split, max_length, dataset_name, tokenizer, rc_aug=False, use_padding=True, add_eos=False):\n",
    "        self.max_length = max_length\n",
    "        self.use_padding = use_padding\n",
    "        self.tokenizer = tokenizer\n",
    "        self.rc_aug = rc_aug\n",
    "        self.add_eos = add_eos\n",
    "\n",
    "        # Download dataset if not available\n",
    "        if not is_downloaded(dataset_name, cache_path=\"./datasets\"):\n",
    "            print(f\"Downloading {dataset_name}...\")\n",
    "            download_dataset(dataset_name, version=0, dest_path=\"./datasets\")\n",
    "        else:\n",
    "            print(f\"Dataset {dataset_name} is already downloaded.\")\n",
    "\n",
    "        # Load data paths and labels\n",
    "        base_path = Path(\"./datasets\") / dataset_name / split\n",
    "        self.all_paths = []\n",
    "        self.all_labels = []\n",
    "        label_mapper = {x.stem: i for i, x in enumerate(base_path.iterdir())}\n",
    "        for label_type in label_mapper.keys():\n",
    "            for x in (base_path / label_type).iterdir():\n",
    "                self.all_paths.append(x)\n",
    "                self.all_labels.append(label_mapper[label_type])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        txt_path = self.all_paths[idx]\n",
    "        with open(txt_path, \"r\") as f:\n",
    "            content = f.read()\n",
    "        x = content\n",
    "        y = self.all_labels[idx]\n",
    "\n",
    "        # Apply reverse complement augmentation\n",
    "        if self.rc_aug and coin_flip():\n",
    "            x = string_reverse_complement(x)\n",
    "\n",
    "        seq = self.tokenizer(x, add_special_tokens=False, padding=\"max_length\" if self.use_padding else None, \n",
    "                             max_length=self.max_length, truncation=True)[\"input_ids\"]\n",
    "\n",
    "        # Add end-of-sequence token\n",
    "        if self.add_eos:\n",
    "            seq.append(self.tokenizer.sep_token_id)\n",
    "\n",
    "        seq = torch.LongTensor(seq)  # Convert to tensor\n",
    "        target = torch.LongTensor([y])\n",
    "        return seq, target\n",
    "\n",
    "\n",
    "# Train Function\n",
    "def train(model, device, train_loader, optimizer, epoch, loss_fn, log_interval=10):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output, target.squeeze())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} \"\n",
    "                  f\"({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
    "\n",
    "\n",
    "# Test Function\n",
    "def test(model, device, test_loader, loss_fn):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += loss_fn(output, target.squeeze()).item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print(f\"\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} \"\n",
    "          f\"({accuracy:.2f}%)\\n\")\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# Main Function to Perform Benchmarking\n",
    "def run_benchmark(task_name):\n",
    "    # Task-specific settings\n",
    "    tasks = {\n",
    "        \"human_enhancers\": {\n",
    "            \"dataset_name\": \"human_enhancers_cohn\",\n",
    "            \"max_length\": 500,\n",
    "            \"num_classes\": 2,\n",
    "        },\n",
    "        \"worm_vs_human\": {\n",
    "            \"dataset_name\": \"demo_human_or_worm\",\n",
    "            \"max_length\": 200,\n",
    "            \"num_classes\": 2,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    if task_name not in tasks:\n",
    "        raise ValueError(f\"Task {task_name} not recognized. Available tasks: {list(tasks.keys())}\")\n",
    "\n",
    "    task_config = tasks[task_name]\n",
    "\n",
    "    # Training settings\n",
    "    num_epochs = 10\n",
    "    batch_size = 128\n",
    "    learning_rate = 6e-4\n",
    "    weight_decay = 0.1\n",
    "    rc_aug = True\n",
    "    add_eos = False\n",
    "\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = CharacterTokenizer(\n",
    "        characters=[\"A\", \"C\", \"G\", \"T\", \"N\"],\n",
    "        model_max_length=task_config[\"max_length\"] + 2,\n",
    "        add_special_tokens=False,\n",
    "        padding_side=\"left\",\n",
    "    )\n",
    "\n",
    "    # Load datasets\n",
    "    train_dataset = GenomicBenchmarkDataset(\n",
    "        split=\"train\",\n",
    "        max_length=task_config[\"max_length\"],\n",
    "        dataset_name=task_config[\"dataset_name\"],\n",
    "        tokenizer=tokenizer,\n",
    "        rc_aug=rc_aug,\n",
    "        add_eos=add_eos,\n",
    "    )\n",
    "\n",
    "    test_dataset = GenomicBenchmarkDataset(\n",
    "        split=\"test\",\n",
    "        max_length=task_config[\"max_length\"],\n",
    "        dataset_name=task_config[\"dataset_name\"],\n",
    "        tokenizer=tokenizer,\n",
    "        rc_aug=rc_aug,\n",
    "        add_eos=add_eos,\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Initialize model\n",
    "    model = HyenaDNAPreTrainedModel.from_pretrained(\n",
    "        \"./checkpoints\",\n",
    "        \"hyenadna-tiny-1k-seqlen\",\n",
    "        download=True,\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        use_head=True,\n",
    "        n_classes=task_config[\"num_classes\"],\n",
    "    )\n",
    "\n",
    "    # Optimizer and Loss Function\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Training Loop\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train(model, device, train_loader, optimizer, epoch, loss_fn)\n",
    "        test(model, device, test_loader, loss_fn)\n",
    "\n",
    "\n",
    "# Run Benchmarks\n",
    "run_benchmark(\"human_enhancers\")  # For Human Enhancers\n",
    "# run_benchmark(\"worm_vs_human\")  # Uncomment for Worm vs. Human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Benchmarks\n",
    "run_benchmark(\"human_enhancers\")  # For Human Enhancers\n",
    "# run_benchmark(\"worm_vs_human\")  # Uncomment for Worm vs. Human"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
